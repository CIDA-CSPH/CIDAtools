---
title: "Additional Options for RStudio/Jupyterlab on the CSPH Biostats Cluster"
author: "Research Tools Committee"
date: "Last Updated: `r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Additional Options for RStudio/Jupyterlab on the CSPH Biostats Cluster}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document will provide optional configuration options for the RStudio and JupyterLab interfaces. 

This project has a [GitHub repository](https://github.com/CIDA-CSPH/Biostats-Cluster-Tools) which containers Apptainer recipes (`.def`) and Dockerfiles for all containers, as well as the internal scripts used by each container.

If you encounter issues using these tools, feel free to [open an issue on the GitHub Issues page](https://github.com/CIDA-CSPH/Biostats-Cluster-Tools/issues).

## Apptainer Containers

The JupyterLab and RStudio instances both run inside of Apptainer containers. 

Apptainer (formerly Singularity) containers are stored as self-contained files with the `.sif` extension.

The benefit of hosting JupyterLab and RStudio within containers is that they offer a consistent environment where all dependencies are installed by default. 

The RStudio and JupyterLab containers are derived from a [base container image](https://github.com/CIDA-CSPH/Biostats-Cluster-Tools/blob/main/containers/RBase/docker/Dockerfile) which holds installation of R, Python, and Julia languages, along with packages/libraries to support installing most other common R/Python libraries.

### Bind Mounts
One important concept when working with containers is a 'bind mount'. Containers are like small Linux installations which run in a separate, read-only filesystem. 

Bind mounts allow us to overlay specific parts of the cluster's filesystem onto the container, so we can read and write our own data files from inside the container. 

By default, Apptainer mounts a few directories including your home directory (`~`), your working directory (`.`) and `/tmp`.

The `jupyter_helper.sh` and `rstudio_helper.sh` scripts also include an additional bind mount:

`--bind /biostats_share:/biostats_share`

which allows us to access the `/biostats_share` directory from inside the container.

If you find yourself unable to access files from within the container, you can add the specific directory as a bind mount by modifying the `*_helper.sh` script to include additional bind mounts:

```
# This command redirects the rest of this script to a sbatch call.
sbatch <<==SBATCH=SCRIPT==
#!/bin/bash
#SBATCH --job-name=jupyter_helper
#SBATCH --output=jupyter_helper.out
#SBATCH --error=jupyter_helper.err
#SBATCH --exclude=csphbiostats.ucdenver.pvt
#SBATCH --mem=64G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8

# Launch the container.
apptainer run \
    --bind /biostats_share:/biostats_share \
    --bind /another_special_directory:/another_special_directory
/biostats_share/hillandr/containers/jupyterlab.sif \
    --notebook-dir "$HOME" \
    --venv "$HOME/my_custom_venv"
==SBATCH=SCRIPT==
```

The line `--bind /another_special_directory:/another_special_directory` make the `/another_special_directory` path available in the container (make sure to use an absolute path).

In general, bind mount arguments look like:

`--bind <local_directory>:<container_path>`

To mount `<local_directory>` path on the cluster to `<container_path` inside the container. 

To reduce confusion, I usually mount each local path to the same path within the container (as I did with `/biostats_share`).

## JupyterLab Options

The JupyterLab container holds an installation of Python 3.13. 

When launched for the first time, the container will create a new `virtualenv` environment in your home directory at `~/jupyterlab_venv`, which will hold all Python dependencies (including JupyterLab itself).

### Using an alternative `venv` for JupyterLab
Although the `~/jupyterlab_venv` is used by default, you can specify a different virtualenv if you wish.

You can do this by modifying the `jupyter_helper.sh` script and passing a value for the `--venv` parameter to the `jupyter.sif` container.

The would look like:

```
# This command redirects the rest of this script to a sbatch call.
sbatch <<==SBATCH=SCRIPT==
#!/bin/bash
#SBATCH --job-name=jupyter_helper
#SBATCH --output=jupyter_helper.out
#SBATCH --error=jupyter_helper.err
#SBATCH --exclude=csphbiostats.ucdenver.pvt
#SBATCH --mem=64G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8

# Launch the container.
apptainer run \
    --bind /biostats_share:/biostats_share \
/biostats_share/hillandr/containers/jupyterlab.sif \
    --notebook-dir "$HOME" \
    --venv "$HOME/my_custom_venv"
==SBATCH=SCRIPT==
```

The second-to-last line `--venv "$HOME/my_custom_venv"` sets the path to the alternative venv.

If this path exists already, the launch script will attempt to activate and use this virtual environment. 

If the path does *not* exist, it will be created and initialized with a default set of packages, which includes JupyterLab.

**NOTE:** If you do attempt to use an pre-existing virtual environment, ensure that JupyterLab is installed *before* running `jupyter_helper.sh`, as this script will fail if no JupyterLab installation is found.

### Setting an Alternative home
By default, the JupyterLab interface will use your `$HOME` (`~`) directory as the launch directory. 

If you wish to use a different directory, you can change the `--notebook-dir` argument in `jupyter_helper.sh` 

## RStudio Options

The RStudio container uses an installation of R 4.5.0, which is the latest version at the time of writing. 

By default, the container comes with many libraries pre-installed, including the `tidyverse` family of packages.

Additionally, the container has many development libraries installed, which should allow you to compile most R packages from source if required.

### Renv Usage
Although the container has many libraries pre-installed, I highly recommend using `renv` in each of your RStudio projects. 

You can run `renv::init()` from an RStudio project to initialize `renv`.

### Changing the R Version

By default, the container uses R-4.5.0, but R-4.4.3 is also installed for compatibility. 

To change the version of R used when launching RStudio, you can change the `R_PATH` variable defined in the `rstudio_helper.sh` script:

```
# Default version of R is 4.5.0.
#R_PATH="/opt/R/4.5.0/bin/R"
R_PATH="/opt/R/4.4.3/bin/R"
```

Changing this variable changes the path passed to the `--r-path` argument later in the script.

Note that these paths are relative to the container, not to the cluster's filesystem. 

Currently the only two supported paths (and versions of R) are:

|Version|Path|
|-------|----|
| **R-4.5.0 (default)** | /opt/R/4.5.0/bin/R |
| R-4.4.3 | /opt/R/4.4.3/bin/R |

If you need an alternative version of R installed, open an issue in the [GitHub repository for this project](https://github.com/CIDA-CSPH/Biostats-Cluster-Tools/issues) and I can add it to the container. 

## Implementation Details

If you are curious as to how the RStudio/JupyterLab interfaces are configured, this section provides a brief explanation.

The challenge with running RStudio/JupyterLab on the cluster is that the RStudio/JupyterLab server needs to run on a compute node, which is not directly acessible (via the network) to your local machine. Additionally, hosting a public web server on these nodes could present a security risk.

```{r, echo=FALSE, out.width="80%", fig.align="center"}
knitr::include_graphics("biostats_hpc_rstudio_diagram.png")
```

To facilitate a secure connection, each RStudio/JupyterLab job creates a link between your (local) computer and the compute node running the job via a pair of SSH tunnels.

The first SSH tunnel is configured locally on your machine (see the [Add SSH Configuration Options](https://cida-csph.github.io/CIDAtools/articles/BIOS_Cluster_RStudio.html#add-ssh-configuration-options) section in the main BIOS Cluster RStudio article).

This SSH tunnel any local traffic on `localhost:8895` or `localhost:8896` to specific Unix sockets (not ports) on the `csphbiostats` head node. We use Unix sockets for this since they:

1. Allow for file-like permissions (i.e. the socket is created and owned by your user, and cannot be accessed by others). 
2. Since Unix sockets are exposed as file-like objects, we can enforce unique naming (each Unix socket uses the user's name as part of the socket path) to prevent port conflicts.

The second SSH tunnel is opened by the compute node. Once your job begins running on the compute node, the RStudio/JupyterLab launch script will:

1. Launch an RStudio/JupyterLab instance advertised on a Unix socket (again, we use Unix sockets instead of ports for the reasons listed above).
2. Open a background SSH tunnel to the `csphbiostats` node, which forwards all traffic on the `csphbiostats`'s Unix socket to the compute node's Unix socket (which is hosting the JupyterLab/RStudio instance).

In order for this to work seamlessly, the compute node needs to be able to SSH into the head node without prompting for a password. The launch script takes care of this by generating a SSH keypair (located at `~/.ssh/cluster_rsa`) which the compute node may use to log in to the head node. 

Together, these two SSH tunnels form a connection between your browser visiting `localhost:8895` and the RStudio/JupyterLab instance on the compute node. 

In summary, the launch script will:
1. Check if the SSH keypair `~/.ssh/cluster_rsa` exists. 
    - If not, the launch script will automatically create this keypair and use it for future job launches.
2. Check if your job is running on the head node `csphbiostats` or a compute node (any node which is not `csphbiostats`)
    - In the event that the job is running on `csphbiostats`, we don't need the second SSH tunnel, as your local machine will already forward traffic directly to the Unix socket on `csphbiostats`.
3. Launch the RStudio/JupyterLab instance. 